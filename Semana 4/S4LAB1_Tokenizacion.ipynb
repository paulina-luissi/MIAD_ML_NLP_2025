{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización de textos  \n",
    "\n",
    "En este notebook aprenderá a tokenizar un texto usando la librería especializada sklearn y [nltk](https://www.nltk.org/).\n",
    "\n",
    "Este notebook tiene la licencia de [Creative Commons Attribution-ShareAlike 3.0 Unported License](http://creativecommons.org/licenses/by-sa/3.0/deed.en_US). Un agradecimiento especial para [Kevin Markham](https://github.com/justmarkham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones Generales:\n",
    "\n",
    "La tokenización es un proceso primordial para la limpieza de datos de texto que permite mejorar el performance de los modelos predictivos de procesamiento de lenguaje natural. Por medio de este notebook deberá tokenizar el texto del set de noticias populares de UCL. Para conocer más detalles de la base puede ingresar al siguiente [vínculo](https://archive.ics.uci.edu/ml/datasets/online+news+popularity#).\n",
    "   \n",
    "Para realizar la actividad, solo siga las indicaciones asociadas a cada celda del notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar base de datos y librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUGERIDO: Descomenta la siguiente linea de código si requieres instalar las libreías básicas utilizadas en este notebook\n",
    "# Si requieres incluir más librerías puedes agregarlas al archivo Semana 4\\requirements.txt\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_web</th>\n",
       "      <th>shares</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>facebo</th>\n",
       "      <th>google</th>\n",
       "      <th>linked</th>\n",
       "      <th>twitte</th>\n",
       "      <th>twitter_followers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seth Fiegerman</td>\n",
       "      <td>http://mashable.com/people/seth-fiegerman/</td>\n",
       "      <td>4900</td>\n",
       "      <td>\\nApple's long and controversial ebook case ha...</td>\n",
       "      <td>The Supreme Court smacked down Apple today</td>\n",
       "      <td>http://www.facebook.com/sfiegerman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.linkedin.com/in/sfiegerman</td>\n",
       "      <td>https://twitter.com/sfiegerman</td>\n",
       "      <td>14300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca Ruiz</td>\n",
       "      <td>http://mashable.com/people/rebecca-ruiz/</td>\n",
       "      <td>1900</td>\n",
       "      <td>Analysis\\n\\n\\n\\n\\n\\nThere is a reason that Don...</td>\n",
       "      <td>Every woman has met a man like Donald Trump</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/rebecca_ruiz</td>\n",
       "      <td>3738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Davina Merchant</td>\n",
       "      <td>http://mashable.com/people/568bdab351984019310...</td>\n",
       "      <td>7000</td>\n",
       "      <td>LONDON - Last month we reported on a dog-sized...</td>\n",
       "      <td>Adorable dog-sized rabbit finally finds his fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://plus.google.com/105525238342980116477?...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scott Gerber</td>\n",
       "      <td>[]</td>\n",
       "      <td>5000</td>\n",
       "      <td>Today's digital marketing experts must have a ...</td>\n",
       "      <td>15 essential skills all digital marketing hire...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Josh Dickey</td>\n",
       "      <td>http://mashable.com/people/joshdickey/</td>\n",
       "      <td>1600</td>\n",
       "      <td>LOS ANGELES — For big, fun, populist popcorn m...</td>\n",
       "      <td>Mashable top 10: 'The Force Awakens' is the be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://plus.google.com/109213469090692520544?...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/JLDlite</td>\n",
       "      <td>11200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                         author_web  shares  \\\n",
       "0   Seth Fiegerman         http://mashable.com/people/seth-fiegerman/    4900   \n",
       "1     Rebecca Ruiz           http://mashable.com/people/rebecca-ruiz/    1900   \n",
       "2  Davina Merchant  http://mashable.com/people/568bdab351984019310...    7000   \n",
       "3     Scott Gerber                                                 []    5000   \n",
       "4      Josh Dickey             http://mashable.com/people/joshdickey/    1600   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nApple's long and controversial ebook case ha...   \n",
       "1  Analysis\\n\\n\\n\\n\\n\\nThere is a reason that Don...   \n",
       "2  LONDON - Last month we reported on a dog-sized...   \n",
       "3  Today's digital marketing experts must have a ...   \n",
       "4  LOS ANGELES — For big, fun, populist popcorn m...   \n",
       "\n",
       "                                               title  \\\n",
       "0         The Supreme Court smacked down Apple today   \n",
       "1        Every woman has met a man like Donald Trump   \n",
       "2  Adorable dog-sized rabbit finally finds his fo...   \n",
       "3  15 essential skills all digital marketing hire...   \n",
       "4  Mashable top 10: 'The Force Awakens' is the be...   \n",
       "\n",
       "                               facebo  \\\n",
       "0  http://www.facebook.com/sfiegerman   \n",
       "1                                 NaN   \n",
       "2                                 NaN   \n",
       "3                                 NaN   \n",
       "4                                 NaN   \n",
       "\n",
       "                                              google  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  https://plus.google.com/105525238342980116477?...   \n",
       "3                                                NaN   \n",
       "4  https://plus.google.com/109213469090692520544?...   \n",
       "\n",
       "                                  linked                            twitte  \\\n",
       "0  http://www.linkedin.com/in/sfiegerman    https://twitter.com/sfiegerman   \n",
       "1                                    NaN  https://twitter.com/rebecca_ruiz   \n",
       "2                                    NaN                               NaN   \n",
       "3                                    NaN                               NaN   \n",
       "4                                    NaN       https://twitter.com/JLDlite   \n",
       "\n",
       "   twitter_followers  \n",
       "0              14300  \n",
       "1               3738  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4              11200  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de datos de archivos .csv\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/mashable_texts.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear varaible de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 10)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       82.000000\n",
       "mean      3090.487805\n",
       "std       8782.031594\n",
       "min        437.000000\n",
       "25%        893.500000\n",
       "50%       1200.000000\n",
       "75%       2275.000000\n",
       "max      63100.000000\n",
       "Name: shares, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separación de variable de interés (y)\n",
    "y = df.shares\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shares\n",
       "1    22\n",
       "0    21\n",
       "3    21\n",
       "2    18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorización de la variable de interés (y)\n",
    "y = pd.cut(y, [0, 893, 1200, 2275, 63200], labels=[0, 1, 2, 3])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de variable de interés en el dataframe\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear variables predictoras X_A - tokenización sin limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X), solo se considera el texto de la noticia\n",
    "X = df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de matrices de documentos usando CountVectorizer a partir de X\n",
    "vect_A = CountVectorizer()\n",
    "X_dtm_A = vect_A.fit_transform(X)\n",
    "temp_A=X_dtm_A.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7969"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización del tamaño del diccionario de palabras con su respectivo ID asignado\n",
    "\n",
    "len(vect_A.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 682),\n",
       " ('long', 4303),\n",
       " ('and', 617),\n",
       " ('controversial', 1747),\n",
       " ('ebook', 2401)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización de los primeros 5 valores del diccionario de palabras \n",
    "# cada key es una palabra y cada valor es la posición en que dicha palabra se encuentra en la matrix \n",
    "list(vect_A.vocabulary_.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 7969)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresión de dimensiones de matriz de documentos donde las filas son documentos y las columnas son términos o tokens\n",
    "X_dtm_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \\nApple's long and controversial ebook case ha...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#primera observación \n",
    "df['text'].iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# tanto en la posición 4303 como en la 2401 el valor en la matriz es 1 porque el texto contiene esas palabras \n",
    "print(temp_A[0, 4303])\n",
    "print(temp_A[0, 2401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 5, 'is': 1, 'long': 2, 'day': 0, 'time': 6, 'no': 3, 'see': 4}\n",
      "[[1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#otro ejemplo \n",
    "\n",
    "X_prueba = [\"this is a long day\", \"long time no see\"]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X_dtm = vect.fit_transform(X_prueba)\n",
    "temp = X_dtm.todense()\n",
    "\n",
    "print(vect.vocabulary_)   # Miramos el índice de cada palabra\n",
    "print(temp[0])            # Vemos la representación vectorial de la primera frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ydwnm50jlu' 'ye' 'yeah' 'year' 'years' 'yec' 'yeezy' 'yellow' 'yelp'\n",
      " 'yep' 'yes' 'yesterday' 'yesweather' 'yet' 'yoga' 'yong' 'york' 'you'\n",
      " 'young' 'younger' 'youngest' 'your' 'yourself' 'youth' 'youtube'\n",
      " 'youtubeduck' 'yup' 'yuyuan' 'yücel' 'zach' 'zaxoqbv487' 'zero'\n",
      " 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictexnxgxmtujcmujanbn'\n",
      " 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1icteymdb4nji3iwplcwpwzw'\n",
      " 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1icti4ohgxnjijcmujanbn'\n",
      " 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictk1mhg1mzqjcmujanbn'\n",
      " 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictu2mhg3ntakzqlqcgc'\n",
      " 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictywmhgzmzgjcmujanbn'\n",
      " 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictexnxgxmtujcmujanbn'\n",
      " 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1icteymdb4nji3iwplcwpwzw'\n",
      " 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1icti4ohgxnjijcmujanbn'\n",
      " 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictk1mhg1mzqjcmujanbn'\n",
      " 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictu2mhg3ntakzqlqcgc'\n",
      " 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictywmhgzmzgjcmujanbn'\n",
      " 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictexnxgxmtujcmujanbn'\n",
      " 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1icteymdb4nji3iwplcwpwzw'\n",
      " 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1icti4ohgxnjijcmujanbn'\n",
      " 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictk1mhg1mzqjcmujanbn'\n",
      " 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictu2mhg3ntakzqlqcgc'\n",
      " 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictywmhgzmzgjcmujanbn']\n"
     ]
    }
   ],
   "source": [
    "# Visualización de 50 términos en el diccionario de palabras\n",
    "print(vect_A.get_feature_names_out()[-150:-100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear variables predictoras X_B - tokenización con limpieza de mayúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de matrices de documentos usando CountVectorizer a partir de X, volviendo todas la palabras en minúscula\n",
    "# a partir del parámetro 'lowercase=False' \n",
    "vect_B = CountVectorizer(lowercase=False)\n",
    "X_dtm_B = vect_B.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 8759)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresión dimensiones de matriz de documentos donde las filas son documentos y las columnas son términos o tokens\n",
    "X_dtm_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weighed' 'weird' 'welcome' 'welcomed' 'welcoming' 'welfare' 'well'\n",
      " 'wells' 'went' 'were' 'weren' 'west' 'western' 'what' 'whatever'\n",
      " 'whatsoever' 'wheel' 'wheelchair' 'wheeliz' 'wheels' 'when' 'where'\n",
      " 'wherever' 'whether' 'which' 'while' 'whistles' 'white' 'who' 'whole'\n",
      " 'wholesome' 'whose' 'why' 'wide' 'widely' 'wider' 'widespread' 'widgets'\n",
      " 'width' 'wife' 'wildest' 'wildly' 'will' 'willing' 'willrahn' 'win'\n",
      " 'wind' 'window' 'windows' 'windscreen']\n"
     ]
    }
   ],
   "source": [
    "# Visualización de 50 términos en el diccionario de palabras\n",
    "print(vect_B.get_feature_names_out()[-150:-100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear variables predictoras X_C - tokenización con limpieza de mayúsculas y usando n-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de matrices de documentos usando CountVectorizer a partir de X y usando n-gramas\n",
    "# a partir del parámetro 'ngram_range=(1, 4)' \n",
    "vect_C = CountVectorizer(lowercase=False, ngram_range=(1, 4))\n",
    "X_dtm_C = vect_C.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 116956)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresión de dimensiones de matriz de documentos, donde las filas son documentos y las columnas son términos o tokens\n",
    "X_dtm_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your scope of knowledge' 'your score' 'your score the'\n",
      " 'your score the more' 'your skills' 'your skills in'\n",
      " 'your skills in variety' 'your skin' 'your skin The'\n",
      " 'your skin The fabric' 'your smart' 'your smart phone'\n",
      " 'your smart phone runs' 'your smartphone' 'your smartphone and'\n",
      " 'your smartphone and go' 'your software' 'your software and'\n",
      " 'your software and that' 'your sorrows' 'your sorrows in'\n",
      " 'your sorrows in bag' 'your specific' 'your specific company'\n",
      " 'your specific company For' 'your startup' 'your startup should'\n",
      " 'your startup should look' 'your tablet' 'your tablet and'\n",
      " 'your tablet and start' 'your tasty' 'your tasty souvenirs'\n",
      " 'your tasty souvenirs South' 'your three' 'your three main'\n",
      " 'your three main movement' 'your time' 'your time and'\n",
      " 'your time and should' 'your time to' 'your time to The' 'your toddler'\n",
      " 'your toddler doesn' 'your toddler doesn have' 'your toolbox'\n",
      " 'your toolbox If' 'your toolbox If you' 'your tush' 'your tush and']\n"
     ]
    }
   ],
   "source": [
    "# Visualización de 50 términos en el diccionario de palabras\n",
    "print(vect_C.get_feature_names_out()[-150:-100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entrenar modelo de predicción con diferentes matrices de palabras (variables predictoras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean      0.369444\n",
       "std       0.158925\n",
       "min       0.111111\n",
       "25%       0.250000\n",
       "50%       0.354167\n",
       "75%       0.500000\n",
       "max       0.625000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición de modelo Naive Bayes para predecir la varaible 'y' y variables predictoras x_A\n",
    "nb = MultinomialNB()\n",
    "pd.Series(cross_val_score(nb, X_dtm_A, y, cv=10)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean      0.368056\n",
       "std       0.165278\n",
       "min       0.111111\n",
       "25%       0.281250\n",
       "50%       0.375000\n",
       "75%       0.486111\n",
       "max       0.625000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición de modelo Naive Bayes para predecir la varaible 'y' y variables predictoras x_B\n",
    "nb = MultinomialNB()\n",
    "pd.Series(cross_val_score(nb, X_dtm_B, y, cv=10)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean      0.352778\n",
       "std       0.118613\n",
       "min       0.125000\n",
       "25%       0.270833\n",
       "50%       0.375000\n",
       "75%       0.427083\n",
       "max       0.500000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición de modelo Naive Bayes para predecir la varaible 'y' y variables predictoras x_B\n",
    "nb = MultinomialNB()\n",
    "pd.Series(cross_val_score(nb, X_dtm_C, y, cv=10)).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
